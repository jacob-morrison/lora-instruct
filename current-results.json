{'llama-2-7b': {'bbh_direct_data': {'word_sorting': 0.225, 'disambiguation_qa': 0.4, 'movie_recommendation': 0.35, 'formal_fallacies': 0.525, 'causal_judgement': 0.45, 'multistep_arithmetic_two': 0.025, 'dyck_languages': 0.275, 'sports_understanding': 0.7, 'hyperbaton': 0.425, 'date_understanding': 0.325, 'web_of_lies': 0.475, 'boolean_expressions': 0.6, 'logical_deduction_three_objects': 0.45, 'snarks': 0.475, 'geometric_shapes': 0.1, 'navigate': 0.475, 'penguins_in_a_table': 0.275, 'ruin_names': 0.25, 'logical_deduction_five_objects': 0.175, 'salient_translation_error_detection': 0.275, 'tracking_shuffled_objects_seven_objects': 0.05, 'tracking_shuffled_objects_five_objects': 0.1, 'temporal_sequences': 0.125, 'tracking_shuffled_objects_three_objects': 0.35, 'logical_deduction_seven_objects': 0.1, 'object_counting': 0.35, 'reasoning_about_colored_objects': 0.275, 'average_exact_match': 0.3185185185185184}, 'bbh_cot_data': {'word_sorting': 0.05, 'disambiguation_qa': 0.325, 'movie_recommendation': 0.725, 'formal_fallacies': 0.475, 'causal_judgement': 0.525, 'multistep_arithmetic_two': 0.0, 'dyck_languages': 0.1, 'sports_understanding': 0.875, 'hyperbaton': 0.55, 'date_understanding': 0.65, 'web_of_lies': 0.55, 'boolean_expressions': 0.7, 'logical_deduction_three_objects': 0.675, 'snarks': 0.475, 'geometric_shapes': 0.325, 'navigate': 0.45, 'penguins_in_a_table': 0.35, 'ruin_names': 0.325, 'logical_deduction_five_objects': 0.325, 'salient_translation_error_detection': 0.15, 'tracking_shuffled_objects_seven_objects': 0.075, 'tracking_shuffled_objects_five_objects': 0.1, 'temporal_sequences': 0.175, 'tracking_shuffled_objects_three_objects': 0.35, 'logical_deduction_seven_objects': 0.225, 'object_counting': 0.55, 'reasoning_about_colored_objects': 0.525, 'average_exact_match': 0.3925925925925926}, 'gsm_direct_data': {'exact_match': 0.08}, 'gsm_cot_data': {'exact_match': 0.12}, 'mmlu_0_shot_data': {'average_acc': 0.32174903859849024, 'subcat_acc': {'math': 0.24154135338345864, 'health': 0.32865853658536587, 'physics': 0.2640625, 'business': 0.39816933638443935, 'biology': 0.3392070484581498, 'chemistry': 0.22772277227722773, 'computer science': 0.3470873786407767, 'economics': 0.2830188679245283, 'engineering': 0.2896551724137931, 'philosophy': 0.2932405566600398, 'other': 0.3622317596566524, 'history': 0.3870967741935484, 'geography': 0.32323232323232326, 'politics': 0.37191358024691357, 'psychology': 0.3621434745030251, 'culture': 0.4006024096385542, 'law': 0.3017583664208735}, 'cat_acc': {'STEM': 0.27634194831013914, 'humanities': 0.3149840595111583, 'social sciences': 0.346766330841729, 'other (business, health, misc.)': 0.3500925354719309}}, 'mmlu_5_shot_data': {'average_acc': 0.341760432986754, 'subcat_acc': {'math': 0.24154135338345864, 'health': 0.35609756097560974, 'physics': 0.26875, 'business': 0.448512585812357, 'biology': 0.35462555066079293, 'chemistry': 0.23432343234323433, 'computer science': 0.34951456310679613, 'economics': 0.3005390835579515, 'engineering': 0.3586206896551724, 'philosophy': 0.3215705765407555, 'other': 0.38626609442060084, 'history': 0.4043010752688172, 'geography': 0.3888888888888889, 'politics': 0.4074074074074074, 'psychology': 0.3759723422644771, 'culture': 0.43373493975903615, 'law': 0.3096993760635281}, 'cat_acc': {'STEM': 0.2839628893306826, 'humanities': 0.3334750265674814, 'social sciences': 0.37146571335716605, 'other (business, health, misc.)': 0.37939543491671807}}, 'toxigen_data': {'trans': 0.922, 'asian': 0.522, 'black': 0.958, 'mexican': 0.922, 'jewish': 0.288, 'native_american': 0.95, 'physical_disability': 0.908, 'lgbtq': 0.836, 'middle_east': 0.926, 'women': 0.886, 'chinese': 0.862, 'latino': 0.318, 'mental_disability': 0.992, 'muslim': 0.894, 'overall': 0.7988571428571428}, 'tydiqa_no_context_data': {'bengali': {'exact_match': 1.0, 'f1': 1.685714285714286}, 'telugu': {'exact_match': 0.0, 'f1': 11.785714285714286}, 'korean': {'exact_match': 11.0, 'f1': 18.720289855072462}, 'arabic': {'exact_match': 3.0, 'f1': 6.681180718594906}, 'russian': {'exact_match': 1.0, 'f1': 5.9553113553113555}, 'indonesian': {'exact_match': 10.0, 'f1': 22.931100397706288}, 'english': {'exact_match': 21.0, 'f1': 38.44995320315571}, 'swahili': {'exact_match': 2.0, 'f1': 2.0}, 'finnish': {'exact_match': 16.0, 'f1': 25.888497794595352}, 'average': {'f1': 14.899751321762741, 'exact_match': 7.222222222222222}}, 'tydiqa_goldp_data': {'english': {'exact_match': 60.0, 'f1': 72.94442296579597}, 'indonesian': {'exact_match': 39.0, 'f1': 57.32277646572257}, 'swahili': {'exact_match': 32.0, 'f1': 41.49572118702554}, 'russian': {'exact_match': 34.0, 'f1': 50.233097857742884}, 'telugu': {'exact_match': 1.0, 'f1': 2.019047619047619}, 'arabic': {'exact_match': 29.0, 'f1': 47.165560140342734}, 'bengali': {'exact_match': 23.0, 'f1': 31.0978354978355}, 'korean': {'exact_match': 48.0, 'f1': 59.30166500166499}, 'finnish': {'exact_match': 38.0, 'f1': 54.236365275186024}, 'average': {'f1': 46.201832445595976, 'exact_match': 33.77777777777778}}}}