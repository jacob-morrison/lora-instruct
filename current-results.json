{'llama-2-7b': {'bbh_direct_data': {'word_sorting': 0.0, 'disambiguation_qa': 0.0, 'movie_recommendation': 0.0, 'formal_fallacies': 0.0, 'causal_judgement': 0.0, 'multistep_arithmetic_two': 0.0, 'dyck_languages': 0.0, 'sports_understanding': 0.0, 'hyperbaton': 0.0, 'date_understanding': 0.0, 'web_of_lies': 0.0, 'boolean_expressions': 0.0, 'logical_deduction_three_objects': 0.0, 'snarks': 0.0, 'geometric_shapes': 0.0, 'navigate': 0.0, 'penguins_in_a_table': 0.0, 'ruin_names': 0.0, 'logical_deduction_five_objects': 0.0, 'salient_translation_error_detection': 0.0, 'tracking_shuffled_objects_seven_objects': 0.0, 'tracking_shuffled_objects_five_objects': 0.0, 'temporal_sequences': 0.0, 'tracking_shuffled_objects_three_objects': 0.0, 'logical_deduction_seven_objects': 0.0, 'object_counting': 0.0, 'reasoning_about_colored_objects': 0.0, 'average_exact_match': 0.0}, 'bbh_cot_data': {'word_sorting': 0.075, 'disambiguation_qa': 0.35, 'movie_recommendation': 0.7, 'formal_fallacies': 0.475, 'causal_judgement': 0.475, 'multistep_arithmetic_two': 0.0, 'dyck_languages': 0.025, 'sports_understanding': 0.85, 'hyperbaton': 0.625, 'date_understanding': 0.65, 'web_of_lies': 0.625, 'boolean_expressions': 0.675, 'logical_deduction_three_objects': 0.7, 'snarks': 0.45, 'geometric_shapes': 0.275, 'navigate': 0.575, 'penguins_in_a_table': 0.375, 'ruin_names': 0.3, 'logical_deduction_five_objects': 0.375, 'salient_translation_error_detection': 0.175, 'tracking_shuffled_objects_seven_objects': 0.1, 'tracking_shuffled_objects_five_objects': 0.175, 'temporal_sequences': 0.125, 'tracking_shuffled_objects_three_objects': 0.4, 'logical_deduction_seven_objects': 0.175, 'object_counting': 0.575, 'reasoning_about_colored_objects': 0.55, 'average_exact_match': 0.40185185185185196}, 'gsm_direct_data': {'exact_match': 0.03}, 'gsm_cot_data': {'exact_match': 0.015}, 'mmlu_0_shot_data': {'average_acc': 0.30828941746190003, 'subcat_acc': {'math': 0.26221804511278196, 'health': 0.3121951219512195, 'physics': 0.265625, 'business': 0.3684210526315789, 'biology': 0.3237885462555066, 'chemistry': 0.22442244224422442, 'computer science': 0.32038834951456313, 'economics': 0.29110512129380056, 'engineering': 0.27586206896551724, 'philosophy': 0.2837972166998012, 'other': 0.33991416309012873, 'history': 0.3279569892473118, 'geography': 0.30303030303030304, 'politics': 0.37037037037037035, 'psychology': 0.337942955920484, 'culture': 0.33433734939759036, 'law': 0.30062393647192287}, 'cat_acc': {'STEM': 0.27700463883366466, 'humanities': 0.29883103081827844, 'social sciences': 0.3308417289567761, 'other (business, health, misc.)': 0.3297347316471314}}, 'mmlu_5_shot_data': {'average_acc': 0.2857142857142857, 'subcat_acc': {'math': 0.2462406015037594, 'health': 0.2774390243902439, 'physics': 0.2515625, 'business': 0.36613272311212813, 'biology': 0.29295154185022027, 'chemistry': 0.23432343234323433, 'computer science': 0.32281553398058255, 'economics': 0.22641509433962265, 'engineering': 0.2620689655172414, 'philosophy': 0.28677932405566603, 'other': 0.3381974248927039, 'history': 0.2924731182795699, 'geography': 0.23737373737373738, 'politics': 0.3317901234567901, 'psychology': 0.28003457216940364, 'culture': 0.31626506024096385, 'law': 0.2819058423142371}, 'cat_acc': {'STEM': 0.2644135188866799, 'humanities': 0.2860786397449522, 'social sciences': 0.27916802079948, 'other (business, health, misc.)': 0.31122763726095004}}, 'toxigen_data': {'trans': 0.928, 'asian': 0.998, 'black': 0.998, 'mexican': 0.776, 'jewish': 0.894, 'native_american': 1.0, 'physical_disability': 0.99, 'lgbtq': 0.794, 'middle_east': 0.934, 'women': 0.964, 'chinese': 0.904, 'latino': 0.82, 'mental_disability': 0.998, 'muslim': 0.982, 'overall': 0.9271428571428572}, 'tydiqa_no_context_data': {'korean': {'exact_match': 0.0, 'f1': 8.567748917748917}, 'indonesian': {'exact_match': 0.0, 'f1': 7.025812628241622}, 'swahili': {'exact_match': 0.0, 'f1': 2.667720059240823}, 'english': {'exact_match': 0.0, 'f1': 8.970169658668473}, 'bengali': {'exact_match': 0.0, 'f1': 0.8571428571428572}, 'telugu': {'exact_match': 0.0, 'f1': 0.28571428571428575}, 'russian': {'exact_match': 0.0, 'f1': 3.942630650371197}, 'finnish': {'exact_match': 0.0, 'f1': 7.0773534654345385}, 'arabic': {'exact_match': 0.0, 'f1': 3.898899631822816}, 'average': {'f1': 4.810354683820615, 'exact_match': 0.0}}, 'tydiqa_goldp_data': {'finnish': {'exact_match': 0.0, 'f1': 16.20360517050465}, 'bengali': {'exact_match': 0.0, 'f1': 17.453990453990457}, 'swahili': {'exact_match': 0.0, 'f1': 12.440376935294768}, 'indonesian': {'exact_match': 0.0, 'f1': 19.351986251143423}, 'english': {'exact_match': 0.0, 'f1': 19.392037288151716}, 'russian': {'exact_match': 0.0, 'f1': 13.665139933317441}, 'telugu': {'exact_match': 0.0, 'f1': 0.8690476190476191}, 'korean': {'exact_match': 0.0, 'f1': 25.489171754171736}, 'arabic': {'exact_match': 0.0, 'f1': 27.627469277803574}, 'average': {'f1': 16.943647187047265, 'exact_match': 0.0}}}}