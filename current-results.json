{
    "tulu_v2": {
        "bbh_direct_data": {
            "word_sorting": 0.2,
            "disambiguation_qa": 0.4,
            "movie_recommendation": 0.325,
            "formal_fallacies": 0.525,
            "causal_judgement": 0.45,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.175,
            "sports_understanding": 0.525,
            "hyperbaton": 0.425,
            "date_understanding": 0.3,
            "web_of_lies": 0.475,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.375,
            "snarks": 0.475,
            "geometric_shapes": 0.3,
            "navigate": 0.525,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.25,
            "salient_translation_error_detection": 0.275,
            "tracking_shuffled_objects_seven_objects": 0.125,
            "tracking_shuffled_objects_five_objects": 0.1,
            "temporal_sequences": 0.075,
            "tracking_shuffled_objects_three_objects": 0.35,
            "logical_deduction_seven_objects": 0.1,
            "object_counting": 0.45,
            "reasoning_about_colored_objects": 0.25,
            "average_exact_match": 0.3259259259259259
        },
        "bbh_cot_data": {
            "word_sorting": 0.075,
            "disambiguation_qa": 0.35,
            "movie_recommendation": 0.7,
            "formal_fallacies": 0.475,
            "causal_judgement": 0.475,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.025,
            "sports_understanding": 0.85,
            "hyperbaton": 0.625,
            "date_understanding": 0.65,
            "web_of_lies": 0.625,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.7,
            "snarks": 0.45,
            "geometric_shapes": 0.275,
            "navigate": 0.575,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.375,
            "salient_translation_error_detection": 0.175,
            "tracking_shuffled_objects_seven_objects": 0.1,
            "tracking_shuffled_objects_five_objects": 0.175,
            "temporal_sequences": 0.125,
            "tracking_shuffled_objects_three_objects": 0.4,
            "logical_deduction_seven_objects": 0.175,
            "object_counting": 0.575,
            "reasoning_about_colored_objects": 0.525,
            "average_exact_match": 0.400925925925926
        },
        "gsm_direct_data": {
            "exact_match": 0.085
        },
        "gsm_cot_data": {
            "exact_match": 0.155
        },
        "mmlu_0_shot_data": {
            "average_acc": 0.3826378008830651,
            "subcat_acc": {
                "math": 0.26785714285714285,
                "health": 0.3957317073170732,
                "physics": 0.2953125,
                "business": 0.4622425629290618,
                "biology": 0.381057268722467,
                "chemistry": 0.24092409240924093,
                "computer science": 0.39563106796116504,
                "economics": 0.31536388140161725,
                "engineering": 0.33793103448275863,
                "philosophy": 0.36381709741550694,
                "other": 0.4609442060085837,
                "history": 0.4731182795698925,
                "geography": 0.41919191919191917,
                "politics": 0.4645061728395062,
                "psychology": 0.46153846153846156,
                "culture": 0.49698795180722893,
                "law": 0.31990924560408396
            },
            "cat_acc": {
                "STEM": 0.30881378396288933,
                "humanities": 0.3689691817215728,
                "social sciences": 0.4280142996425089,
                "other (business, health, misc.)": 0.4281307834669957
            }
        },
        "mmlu_5_shot_data": {
            "average_acc": 0.47336561743341404,
            "subcat_acc": {
                "math": 0.31296992481203006,
                "health": 0.4945121951219512,
                "physics": 0.3796875,
                "business": 0.6727688787185355,
                "biology": 0.552863436123348,
                "chemistry": 0.36633663366336633,
                "computer science": 0.40048543689320387,
                "economics": 0.42183288409703507,
                "engineering": 0.43448275862068964,
                "philosophy": 0.4423459244532803,
                "other": 0.5914163090128756,
                "history": 0.5193548387096775,
                "geography": 0.6616161616161617,
                "politics": 0.5462962962962963,
                "psychology": 0.6248919619706137,
                "culture": 0.6234939759036144,
                "law": 0.33238797504254114
            },
            "cat_acc": {
                "STEM": 0.3863485752153744,
                "humanities": 0.416365568544102,
                "social sciences": 0.5615859603509912,
                "other (business, health, misc.)": 0.5533621221468229
            }
        },
        "toxigen_data": {
            "trans": 0.76,
            "asian": 0.998,
            "black": 0.998,
            "mexican": 0.586,
            "jewish": 0.878,
            "native_american": 0.88,
            "physical_disability": 0.83,
            "lgbtq": 0.656,
            "middle_east": 0.786,
            "women": 0.624,
            "chinese": 0.666,
            "latino": 0.594,
            "mental_disability": 0.936,
            "muslim": 0.916,
            "overall": 0.7934285714285715
        },
        "tydiqa_no_context_data": {
            "arabic": {
                "exact_match": 1.0,
                "f1": 4.132565657086941
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.28571428571428575
            },
            "russian": {
                "exact_match": 4.0,
                "f1": 8.969444444444445
            },
            "bengali": {
                "exact_match": 1.0,
                "f1": 1.4857142857142858
            },
            "finnish": {
                "exact_match": 17.0,
                "f1": 22.37061504541157
            },
            "korean": {
                "exact_match": 14.0,
                "f1": 19.990816326530616
            },
            "indonesian": {
                "exact_match": 7.0,
                "f1": 19.860783376973192
            },
            "english": {
                "exact_match": 26.0,
                "f1": 37.99386159248388
            },
            "swahili": {
                "exact_match": 6.0,
                "f1": 7.404040404040404
            },
            "average": {
                "f1": 13.610395046488847,
                "exact_match": 8.444444444444445
            }
        },
        "tydiqa_goldp_data": {
            "finnish": {
                "exact_match": 41.0,
                "f1": 54.23299833686031
            },
            "swahili": {
                "exact_match": 41.0,
                "f1": 51.821466456249055
            },
            "korean": {
                "exact_match": 40.0,
                "f1": 52.18686868686868
            },
            "arabic": {
                "exact_match": 17.0,
                "f1": 40.93295462787959
            },
            "indonesian": {
                "exact_match": 47.0,
                "f1": 62.62093223422043
            },
            "english": {
                "exact_match": 50.0,
                "f1": 68.85843055820656
            },
            "russian": {
                "exact_match": 23.0,
                "f1": 50.19171406775192
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.7333333333333334
            },
            "bengali": {
                "exact_match": 21.0,
                "f1": 30.9527417027417
            },
            "average": {
                "f1": 45.83682666712351,
                "exact_match": 31.11111111111111
            }
        }
    },
    "code_alpaca_filtered": {
        "bbh_direct_data": {
            "word_sorting": 0.2,
            "disambiguation_qa": 0.4,
            "movie_recommendation": 0.325,
            "formal_fallacies": 0.525,
            "causal_judgement": 0.45,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.175,
            "sports_understanding": 0.525,
            "hyperbaton": 0.425,
            "date_understanding": 0.3,
            "web_of_lies": 0.475,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.375,
            "snarks": 0.475,
            "geometric_shapes": 0.3,
            "navigate": 0.525,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.25,
            "salient_translation_error_detection": 0.275,
            "tracking_shuffled_objects_seven_objects": 0.125,
            "tracking_shuffled_objects_five_objects": 0.1,
            "temporal_sequences": 0.075,
            "tracking_shuffled_objects_three_objects": 0.35,
            "logical_deduction_seven_objects": 0.1,
            "object_counting": 0.45,
            "reasoning_about_colored_objects": 0.25,
            "average_exact_match": 0.3259259259259259
        },
        "bbh_cot_data": {
            "word_sorting": 0.075,
            "disambiguation_qa": 0.35,
            "movie_recommendation": 0.7,
            "formal_fallacies": 0.475,
            "causal_judgement": 0.475,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.025,
            "sports_understanding": 0.85,
            "hyperbaton": 0.625,
            "date_understanding": 0.65,
            "web_of_lies": 0.625,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.7,
            "snarks": 0.45,
            "geometric_shapes": 0.275,
            "navigate": 0.575,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.375,
            "salient_translation_error_detection": 0.175,
            "tracking_shuffled_objects_seven_objects": 0.1,
            "tracking_shuffled_objects_five_objects": 0.175,
            "temporal_sequences": 0.125,
            "tracking_shuffled_objects_three_objects": 0.4,
            "logical_deduction_seven_objects": 0.175,
            "object_counting": 0.575,
            "reasoning_about_colored_objects": 0.525,
            "average_exact_match": 0.400925925925926
        },
        "gsm_direct_data": {
            "exact_match": 0.085
        },
        "gsm_cot_data": {
            "exact_match": 0.155
        },
        "mmlu_0_shot_data": {
            "average_acc": 0.32488249537102976,
            "subcat_acc": {
                "math": 0.24718045112781956,
                "health": 0.33902439024390246,
                "physics": 0.278125,
                "business": 0.38672768878718533,
                "biology": 0.3458149779735683,
                "chemistry": 0.22772277227722773,
                "computer science": 0.3470873786407767,
                "economics": 0.2816711590296496,
                "engineering": 0.33793103448275863,
                "philosophy": 0.3106361829025845,
                "other": 0.3587982832618026,
                "history": 0.3763440860215054,
                "geography": 0.35353535353535354,
                "politics": 0.36728395061728397,
                "psychology": 0.34917891097666376,
                "culture": 0.4367469879518072,
                "law": 0.2943845717526943
            },
            "cat_acc": {
                "STEM": 0.2846255798542081,
                "humanities": 0.3175345377258236,
                "social sciences": 0.34644133896652585,
                "other (business, health, misc.)": 0.35256014805675506
            }
        },
        "mmlu_5_shot_data": {
            "average_acc": 0.3288705312633528,
            "subcat_acc": {
                "math": 0.24906015037593984,
                "health": 0.3475609756097561,
                "physics": 0.2828125,
                "business": 0.39359267734553777,
                "biology": 0.3303964757709251,
                "chemistry": 0.23102310231023102,
                "computer science": 0.3422330097087379,
                "economics": 0.2776280323450135,
                "engineering": 0.3448275862068966,
                "philosophy": 0.31411530815109345,
                "other": 0.36738197424892705,
                "history": 0.3709677419354839,
                "geography": 0.41414141414141414,
                "politics": 0.37962962962962965,
                "psychology": 0.36646499567847884,
                "culture": 0.4036144578313253,
                "law": 0.29608621667612023
            },
            "cat_acc": {
                "STEM": 0.2839628893306826,
                "humanities": 0.3185972369819341,
                "social sciences": 0.354891127721807,
                "other (business, health, misc.)": 0.3608883405305367
            }
        },
        "toxigen_data": {
            "trans": 0.76,
            "asian": 0.998,
            "black": 0.998,
            "mexican": 0.586,
            "jewish": 0.878,
            "native_american": 0.88,
            "physical_disability": 0.83,
            "lgbtq": 0.656,
            "middle_east": 0.786,
            "women": 0.624,
            "chinese": 0.666,
            "latino": 0.594,
            "mental_disability": 0.936,
            "muslim": 0.916,
            "overall": 0.7934285714285715
        },
        "tydiqa_no_context_data": {
            "arabic": {
                "exact_match": 1.0,
                "f1": 4.132565657086941
            },
            "bengali": {
                "exact_match": 2.0,
                "f1": 2.5357142857142856
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.0
            },
            "korean": {
                "exact_match": 4.0,
                "f1": 8.233333333333334
            },
            "russian": {
                "exact_match": 1.0,
                "f1": 4.671428571428571
            },
            "indonesian": {
                "exact_match": 9.0,
                "f1": 22.01531297461877
            },
            "swahili": {
                "exact_match": 4.0,
                "f1": 7.18210008945303
            },
            "english": {
                "exact_match": 23.0,
                "f1": 36.98261947203123
            },
            "finnish": {
                "exact_match": 17.0,
                "f1": 27.059654964918124
            },
            "average": {
                "f1": 12.534747705398253,
                "exact_match": 6.777777777777778
            }
        },
        "tydiqa_goldp_data": {
            "korean": {
                "exact_match": 57.0,
                "f1": 67.36790801790801
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 1.269047619047619
            },
            "english": {
                "exact_match": 48.0,
                "f1": 69.22847623137098
            },
            "arabic": {
                "exact_match": 25.0,
                "f1": 47.915051652382054
            },
            "finnish": {
                "exact_match": 47.0,
                "f1": 57.995816021663956
            },
            "swahili": {
                "exact_match": 22.0,
                "f1": 32.18513169474267
            },
            "indonesian": {
                "exact_match": 38.0,
                "f1": 51.064307691456015
            },
            "russian": {
                "exact_match": 27.0,
                "f1": 52.06203726131989
            },
            "bengali": {
                "exact_match": 21.0,
                "f1": 30.228860028860026
            },
            "average": {
                "f1": 45.479626246527914,
                "exact_match": 31.666666666666668
            }
        }
    },
    "cot_filtered": {
        "bbh_direct_data": {
            "word_sorting": 0.2,
            "disambiguation_qa": 0.4,
            "movie_recommendation": 0.325,
            "formal_fallacies": 0.525,
            "causal_judgement": 0.45,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.175,
            "sports_understanding": 0.525,
            "hyperbaton": 0.425,
            "date_understanding": 0.3,
            "web_of_lies": 0.475,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.375,
            "snarks": 0.475,
            "geometric_shapes": 0.3,
            "navigate": 0.525,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.25,
            "salient_translation_error_detection": 0.275,
            "tracking_shuffled_objects_seven_objects": 0.125,
            "tracking_shuffled_objects_five_objects": 0.1,
            "temporal_sequences": 0.075,
            "tracking_shuffled_objects_three_objects": 0.35,
            "logical_deduction_seven_objects": 0.1,
            "object_counting": 0.45,
            "reasoning_about_colored_objects": 0.25,
            "average_exact_match": 0.3259259259259259
        },
        "bbh_cot_data": {
            "word_sorting": 0.075,
            "disambiguation_qa": 0.35,
            "movie_recommendation": 0.7,
            "formal_fallacies": 0.475,
            "causal_judgement": 0.475,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.025,
            "sports_understanding": 0.85,
            "hyperbaton": 0.625,
            "date_understanding": 0.65,
            "web_of_lies": 0.625,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.7,
            "snarks": 0.45,
            "geometric_shapes": 0.275,
            "navigate": 0.575,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.375,
            "salient_translation_error_detection": 0.175,
            "tracking_shuffled_objects_seven_objects": 0.1,
            "tracking_shuffled_objects_five_objects": 0.175,
            "temporal_sequences": 0.125,
            "tracking_shuffled_objects_three_objects": 0.4,
            "logical_deduction_seven_objects": 0.175,
            "object_counting": 0.575,
            "reasoning_about_colored_objects": 0.525,
            "average_exact_match": 0.400925925925926
        },
        "gsm_direct_data": {
            "exact_match": 0.085
        },
        "gsm_cot_data": {
            "exact_match": 0.155
        },
        "mmlu_0_shot_data": {
            "average_acc": 0.32901296111665007,
            "subcat_acc": {
                "math": 0.23684210526315788,
                "health": 0.3432926829268293,
                "physics": 0.2734375,
                "business": 0.43935926773455375,
                "biology": 0.3436123348017621,
                "chemistry": 0.2079207920792079,
                "computer science": 0.33495145631067963,
                "economics": 0.2857142857142857,
                "engineering": 0.2896551724137931,
                "philosophy": 0.3036779324055666,
                "other": 0.38626609442060084,
                "history": 0.3935483870967742,
                "geography": 0.35858585858585856,
                "politics": 0.37808641975308643,
                "psychology": 0.3742437337942956,
                "culture": 0.3825301204819277,
                "law": 0.2972206466250709
            },
            "cat_acc": {
                "STEM": 0.2736911862160371,
                "humanities": 0.3190223166843783,
                "social sciences": 0.35359116022099446,
                "other (business, health, misc.)": 0.3716841455891425
            }
        },
        "mmlu_5_shot_data": {
            "average_acc": 0.3404785643070788,
            "subcat_acc": {
                "math": 0.26033834586466165,
                "health": 0.3518292682926829,
                "physics": 0.29375,
                "business": 0.42105263157894735,
                "biology": 0.36123348017621143,
                "chemistry": 0.23102310231023102,
                "computer science": 0.33495145631067963,
                "economics": 0.29514824797843664,
                "engineering": 0.30344827586206896,
                "philosophy": 0.323558648111332,
                "other": 0.39914163090128757,
                "history": 0.4032258064516129,
                "geography": 0.4090909090909091,
                "politics": 0.37808641975308643,
                "psychology": 0.3958513396715644,
                "culture": 0.3795180722891566,
                "law": 0.2943845717526943
            },
            "cat_acc": {
                "STEM": 0.29191517561298874,
                "humanities": 0.3283740701381509,
                "social sciences": 0.3669158271043224,
                "other (business, health, misc.)": 0.37816162862430597
            }
        },
        "toxigen_data": {
            "trans": 0.76,
            "asian": 0.998,
            "black": 0.998,
            "mexican": 0.586,
            "jewish": 0.878,
            "native_american": 0.88,
            "physical_disability": 0.83,
            "lgbtq": 0.656,
            "middle_east": 0.786,
            "women": 0.624,
            "chinese": 0.666,
            "latino": 0.594,
            "mental_disability": 0.936,
            "muslim": 0.916,
            "overall": 0.7934285714285715
        },
        "tydiqa_no_context_data": {
            "korean": {
                "exact_match": 16.0,
                "f1": 22.84081632653061
            },
            "english": {
                "exact_match": 24.0,
                "f1": 34.27490650607711
            },
            "finnish": {
                "exact_match": 13.0,
                "f1": 19.879206349206353
            },
            "swahili": {
                "exact_match": 1.0,
                "f1": 3.865384615384616
            },
            "indonesian": {
                "exact_match": 13.0,
                "f1": 24.88719499081515
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.28571428571428575
            },
            "bengali": {
                "exact_match": 0.0,
                "f1": 0.7357142857142859
            },
            "russian": {
                "exact_match": 5.0,
                "f1": 9.783333333333331
            },
            "arabic": {
                "exact_match": 1.0,
                "f1": 4.6111571489974565
            },
            "average": {
                "f1": 13.462603093530355,
                "exact_match": 8.11111111111111
            }
        },
        "tydiqa_goldp_data": {
            "english": {
                "exact_match": 48.0,
                "f1": 63.19258792037145
            },
            "bengali": {
                "exact_match": 20.0,
                "f1": 30.960678210678207
            },
            "russian": {
                "exact_match": 27.0,
                "f1": 42.922364175010934
            },
            "swahili": {
                "exact_match": 28.0,
                "f1": 41.23626459129518
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.6666666666666665
            },
            "korean": {
                "exact_match": 51.0,
                "f1": 61.968154068154064
            },
            "finnish": {
                "exact_match": 57.0,
                "f1": 70.13324576176971
            },
            "indonesian": {
                "exact_match": 46.0,
                "f1": 61.826756995226816
            },
            "arabic": {
                "exact_match": 30.0,
                "f1": 51.58194677848734
            },
            "average": {
                "f1": 47.16540724085115,
                "exact_match": 34.111111111111114
            }
        }
    },
    "flan_v2_filtered": {
        "bbh_direct_data": {
            "word_sorting": 0.2,
            "disambiguation_qa": 0.4,
            "movie_recommendation": 0.325,
            "formal_fallacies": 0.525,
            "causal_judgement": 0.45,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.175,
            "sports_understanding": 0.525,
            "hyperbaton": 0.425,
            "date_understanding": 0.3,
            "web_of_lies": 0.475,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.375,
            "snarks": 0.475,
            "geometric_shapes": 0.3,
            "navigate": 0.525,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.25,
            "salient_translation_error_detection": 0.275,
            "tracking_shuffled_objects_seven_objects": 0.125,
            "tracking_shuffled_objects_five_objects": 0.1,
            "temporal_sequences": 0.075,
            "tracking_shuffled_objects_three_objects": 0.35,
            "logical_deduction_seven_objects": 0.1,
            "object_counting": 0.45,
            "reasoning_about_colored_objects": 0.25,
            "average_exact_match": 0.3259259259259259
        },
        "bbh_cot_data": {
            "word_sorting": 0.075,
            "disambiguation_qa": 0.35,
            "movie_recommendation": 0.7,
            "formal_fallacies": 0.475,
            "causal_judgement": 0.475,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.025,
            "sports_understanding": 0.85,
            "hyperbaton": 0.625,
            "date_understanding": 0.65,
            "web_of_lies": 0.625,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.7,
            "snarks": 0.45,
            "geometric_shapes": 0.275,
            "navigate": 0.575,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.375,
            "salient_translation_error_detection": 0.175,
            "tracking_shuffled_objects_seven_objects": 0.1,
            "tracking_shuffled_objects_five_objects": 0.175,
            "temporal_sequences": 0.125,
            "tracking_shuffled_objects_three_objects": 0.4,
            "logical_deduction_seven_objects": 0.175,
            "object_counting": 0.575,
            "reasoning_about_colored_objects": 0.525,
            "average_exact_match": 0.400925925925926
        },
        "gsm_direct_data": {
            "exact_match": 0.085
        },
        "gsm_cot_data": {
            "exact_match": 0.155
        },
        "mmlu_0_shot_data": {
            "average_acc": 0.35436547500356075,
            "subcat_acc": {
                "math": 0.24530075187969924,
                "health": 0.3524390243902439,
                "physics": 0.2765625,
                "business": 0.43935926773455375,
                "biology": 0.3722466960352423,
                "chemistry": 0.22442244224422442,
                "computer science": 0.3737864077669903,
                "economics": 0.3059299191374663,
                "engineering": 0.3586206896551724,
                "philosophy": 0.3320079522862823,
                "other": 0.40429184549356223,
                "history": 0.4290322580645161,
                "geography": 0.40404040404040403,
                "politics": 0.4243827160493827,
                "psychology": 0.41745894554883317,
                "culture": 0.46987951807228917,
                "law": 0.3210436755530346
            },
            "cat_acc": {
                "STEM": 0.29191517561298874,
                "humanities": 0.3470775770456961,
                "social sciences": 0.39681507962300944,
                "other (business, health, misc.)": 0.3827884022208513
            }
        },
        "mmlu_5_shot_data": {
            "average_acc": 0.37131462754593364,
            "subcat_acc": {
                "math": 0.25281954887218044,
                "health": 0.38170731707317074,
                "physics": 0.3453125,
                "business": 0.4782608695652174,
                "biology": 0.3700440528634361,
                "chemistry": 0.23432343234323433,
                "computer science": 0.3422330097087379,
                "economics": 0.3005390835579515,
                "engineering": 0.31724137931034485,
                "philosophy": 0.3742544731610338,
                "other": 0.4188841201716738,
                "history": 0.4505376344086022,
                "geography": 0.4696969696969697,
                "politics": 0.44598765432098764,
                "psychology": 0.4191875540190147,
                "culture": 0.39759036144578314,
                "law": 0.3295519001701645
            },
            "cat_acc": {
                "STEM": 0.3035122597746852,
                "humanities": 0.37258235919234856,
                "social sciences": 0.39714007149821257,
                "other (business, health, misc.)": 0.4080814312152992
            }
        },
        "toxigen_data": {
            "trans": 0.76,
            "asian": 0.998,
            "black": 0.998,
            "mexican": 0.586,
            "jewish": 0.878,
            "native_american": 0.88,
            "physical_disability": 0.83,
            "lgbtq": 0.656,
            "middle_east": 0.786,
            "women": 0.624,
            "chinese": 0.666,
            "latino": 0.594,
            "mental_disability": 0.936,
            "muslim": 0.916,
            "overall": 0.7934285714285715
        },
        "tydiqa_no_context_data": {
            "korean": {
                "exact_match": 11.0,
                "f1": 17.16125541125541
            },
            "arabic": {
                "exact_match": 1.0,
                "f1": 4.487589044941986
            },
            "swahili": {
                "exact_match": 4.0,
                "f1": 4.142857142857142
            },
            "bengali": {
                "exact_match": 1.0,
                "f1": 1.2857142857142858
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.33333333333333326
            },
            "indonesian": {
                "exact_match": 11.0,
                "f1": 22.41833705776804
            },
            "russian": {
                "exact_match": 7.0,
                "f1": 14.405194805194808
            },
            "english": {
                "exact_match": 27.0,
                "f1": 39.71511878105593
            },
            "finnish": {
                "exact_match": 12.0,
                "f1": 20.219746431067183
            },
            "average": {
                "f1": 13.796571810354234,
                "exact_match": 8.222222222222221
            }
        },
        "tydiqa_goldp_data": {
            "english": {
                "exact_match": 49.0,
                "f1": 67.981341049293
            },
            "bengali": {
                "exact_match": 21.0,
                "f1": 31.048196248196245
            },
            "russian": {
                "exact_match": 24.0,
                "f1": 39.823293870791886
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.33333333333333326
            },
            "indonesian": {
                "exact_match": 44.0,
                "f1": 63.202379665209484
            },
            "korean": {
                "exact_match": 43.0,
                "f1": 54.439278390782135
            },
            "arabic": {
                "exact_match": 24.0,
                "f1": 48.530353847117546
            },
            "finnish": {
                "exact_match": 48.0,
                "f1": 61.63854287474977
            },
            "swahili": {
                "exact_match": 22.0,
                "f1": 37.64862343518329
            },
            "average": {
                "f1": 44.96059363496185,
                "exact_match": 30.555555555555557
            }
        }
    },
    "gpt4_alpaca_filtered": {
        "bbh_direct_data": {
            "word_sorting": 0.2,
            "disambiguation_qa": 0.4,
            "movie_recommendation": 0.325,
            "formal_fallacies": 0.525,
            "causal_judgement": 0.45,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.175,
            "sports_understanding": 0.525,
            "hyperbaton": 0.425,
            "date_understanding": 0.3,
            "web_of_lies": 0.475,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.375,
            "snarks": 0.475,
            "geometric_shapes": 0.3,
            "navigate": 0.525,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.25,
            "salient_translation_error_detection": 0.275,
            "tracking_shuffled_objects_seven_objects": 0.125,
            "tracking_shuffled_objects_five_objects": 0.1,
            "temporal_sequences": 0.075,
            "tracking_shuffled_objects_three_objects": 0.35,
            "logical_deduction_seven_objects": 0.1,
            "object_counting": 0.45,
            "reasoning_about_colored_objects": 0.25,
            "average_exact_match": 0.3259259259259259
        },
        "bbh_cot_data": {
            "word_sorting": 0.075,
            "disambiguation_qa": 0.35,
            "movie_recommendation": 0.7,
            "formal_fallacies": 0.475,
            "causal_judgement": 0.475,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.025,
            "sports_understanding": 0.85,
            "hyperbaton": 0.625,
            "date_understanding": 0.65,
            "web_of_lies": 0.625,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.7,
            "snarks": 0.45,
            "geometric_shapes": 0.275,
            "navigate": 0.575,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.375,
            "salient_translation_error_detection": 0.175,
            "tracking_shuffled_objects_seven_objects": 0.1,
            "tracking_shuffled_objects_five_objects": 0.175,
            "temporal_sequences": 0.125,
            "tracking_shuffled_objects_three_objects": 0.4,
            "logical_deduction_seven_objects": 0.175,
            "object_counting": 0.575,
            "reasoning_about_colored_objects": 0.525,
            "average_exact_match": 0.400925925925926
        },
        "gsm_direct_data": {
            "exact_match": 0.085
        },
        "gsm_cot_data": {
            "exact_match": 0.155
        },
        "mmlu_0_shot_data": {
            "average_acc": 0.3196125907990315,
            "subcat_acc": {
                "math": 0.23590225563909775,
                "health": 0.3280487804878049,
                "physics": 0.2734375,
                "business": 0.40274599542334094,
                "biology": 0.3392070484581498,
                "chemistry": 0.18151815181518152,
                "computer science": 0.3300970873786408,
                "economics": 0.24528301886792453,
                "engineering": 0.32413793103448274,
                "philosophy": 0.3061630218687873,
                "other": 0.3648068669527897,
                "history": 0.3774193548387097,
                "geography": 0.3484848484848485,
                "politics": 0.37191358024691357,
                "psychology": 0.35695764909248057,
                "culture": 0.42771084337349397,
                "law": 0.2932501418037436
            },
            "cat_acc": {
                "STEM": 0.2710404241219351,
                "humanities": 0.31540913921360253,
                "social sciences": 0.34026649333766656,
                "other (business, health, misc.)": 0.351326341764343
            }
        },
        "mmlu_5_shot_data": {
            "average_acc": 0.3364193134881071,
            "subcat_acc": {
                "math": 0.25375939849624063,
                "health": 0.35853658536585364,
                "physics": 0.2734375,
                "business": 0.38443935926773454,
                "biology": 0.32158590308370044,
                "chemistry": 0.23102310231023102,
                "computer science": 0.3470873786407767,
                "economics": 0.261455525606469,
                "engineering": 0.36551724137931035,
                "philosophy": 0.32852882703777336,
                "other": 0.3725321888412017,
                "history": 0.4,
                "geography": 0.4090909090909091,
                "politics": 0.42592592592592593,
                "psychology": 0.3802938634399309,
                "culture": 0.4246987951807229,
                "law": 0.290414066931367
            },
            "cat_acc": {
                "STEM": 0.2839628893306826,
                "humanities": 0.3283740701381509,
                "social sciences": 0.36789080272993174,
                "other (business, health, misc.)": 0.36705737199259714
            }
        },
        "toxigen_data": {
            "trans": 0.76,
            "asian": 0.998,
            "black": 0.998,
            "mexican": 0.586,
            "jewish": 0.878,
            "native_american": 0.88,
            "physical_disability": 0.83,
            "lgbtq": 0.656,
            "middle_east": 0.786,
            "women": 0.624,
            "chinese": 0.666,
            "latino": 0.594,
            "mental_disability": 0.936,
            "muslim": 0.916,
            "overall": 0.7934285714285715
        },
        "tydiqa_no_context_data": {
            "english": {
                "exact_match": 20.0,
                "f1": 32.313383266350016
            },
            "swahili": {
                "exact_match": 4.0,
                "f1": 5.310411810411811
            },
            "indonesian": {
                "exact_match": 7.0,
                "f1": 20.32504771307466
            },
            "korean": {
                "exact_match": 13.0,
                "f1": 18.2
            },
            "bengali": {
                "exact_match": 1.0,
                "f1": 1.7857142857142858
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.0
            },
            "finnish": {
                "exact_match": 16.0,
                "f1": 22.058673693456303
            },
            "arabic": {
                "exact_match": 0.0,
                "f1": 3.364206530383001
            },
            "russian": {
                "exact_match": 4.0,
                "f1": 14.846775446775448
            },
            "average": {
                "f1": 13.133801416240614,
                "exact_match": 7.222222222222222
            }
        },
        "tydiqa_goldp_data": {
            "english": {
                "exact_match": 50.0,
                "f1": 64.97126053488958
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 1.0666666666666667
            },
            "arabic": {
                "exact_match": 14.0,
                "f1": 43.379096361648905
            },
            "finnish": {
                "exact_match": 46.0,
                "f1": 60.19888675606969
            },
            "russian": {
                "exact_match": 31.0,
                "f1": 53.88751346692522
            },
            "indonesian": {
                "exact_match": 41.0,
                "f1": 63.51686891464862
            },
            "swahili": {
                "exact_match": 28.0,
                "f1": 41.85747206010364
            },
            "korean": {
                "exact_match": 7.0,
                "f1": 20.58948643948644
            },
            "bengali": {
                "exact_match": 21.0,
                "f1": 31.49617604617605
            },
            "average": {
                "f1": 42.329269694068316,
                "exact_match": 26.444444444444443
            }
        }
    },
    "hard_coded_filtered": {
        "bbh_direct_data": {
            "word_sorting": 0.2,
            "disambiguation_qa": 0.4,
            "movie_recommendation": 0.325,
            "formal_fallacies": 0.525,
            "causal_judgement": 0.45,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.175,
            "sports_understanding": 0.525,
            "hyperbaton": 0.425,
            "date_understanding": 0.3,
            "web_of_lies": 0.475,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.375,
            "snarks": 0.475,
            "geometric_shapes": 0.3,
            "navigate": 0.525,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.25,
            "salient_translation_error_detection": 0.275,
            "tracking_shuffled_objects_seven_objects": 0.125,
            "tracking_shuffled_objects_five_objects": 0.1,
            "temporal_sequences": 0.075,
            "tracking_shuffled_objects_three_objects": 0.35,
            "logical_deduction_seven_objects": 0.1,
            "object_counting": 0.45,
            "reasoning_about_colored_objects": 0.25,
            "average_exact_match": 0.3259259259259259
        },
        "bbh_cot_data": {
            "word_sorting": 0.075,
            "disambiguation_qa": 0.35,
            "movie_recommendation": 0.7,
            "formal_fallacies": 0.475,
            "causal_judgement": 0.475,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.025,
            "sports_understanding": 0.85,
            "hyperbaton": 0.625,
            "date_understanding": 0.65,
            "web_of_lies": 0.625,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.7,
            "snarks": 0.45,
            "geometric_shapes": 0.275,
            "navigate": 0.575,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.375,
            "salient_translation_error_detection": 0.175,
            "tracking_shuffled_objects_seven_objects": 0.1,
            "tracking_shuffled_objects_five_objects": 0.175,
            "temporal_sequences": 0.125,
            "tracking_shuffled_objects_three_objects": 0.4,
            "logical_deduction_seven_objects": 0.175,
            "object_counting": 0.575,
            "reasoning_about_colored_objects": 0.525,
            "average_exact_match": 0.400925925925926
        },
        "gsm_direct_data": {
            "exact_match": 0.085
        },
        "gsm_cot_data": {
            "exact_match": 0.155
        },
        "mmlu_0_shot_data": {
            "average_acc": 0.28863409770687937,
            "subcat_acc": {
                "math": 0.25,
                "health": 0.28841463414634144,
                "physics": 0.24375,
                "business": 0.33867276887871856,
                "biology": 0.2907488986784141,
                "chemistry": 0.2145214521452145,
                "computer science": 0.3325242718446602,
                "economics": 0.24797843665768193,
                "engineering": 0.30344827586206896,
                "philosophy": 0.26938369781312127,
                "other": 0.30300429184549355,
                "history": 0.3279569892473118,
                "geography": 0.2878787878787879,
                "politics": 0.32253086419753085,
                "psychology": 0.2990492653414002,
                "culture": 0.37650602409638556,
                "law": 0.28984685195689164
            },
            "cat_acc": {
                "STEM": 0.26507620941020543,
                "humanities": 0.28862911795961743,
                "social sciences": 0.29931751706207343,
                "other (business, health, misc.)": 0.3004318322023442
            }
        },
        "mmlu_5_shot_data": {
            "average_acc": 0.28970232160660875,
            "subcat_acc": {
                "math": 0.24342105263157895,
                "health": 0.2939024390243902,
                "physics": 0.2515625,
                "business": 0.34553775743707094,
                "biology": 0.2973568281938326,
                "chemistry": 0.2607260726072607,
                "computer science": 0.3058252427184466,
                "economics": 0.2803234501347709,
                "engineering": 0.30344827586206896,
                "philosophy": 0.28876739562624254,
                "other": 0.30300429184549355,
                "history": 0.2946236559139785,
                "geography": 0.26262626262626265,
                "politics": 0.31790123456790126,
                "psychology": 0.2990492653414002,
                "culture": 0.35843373493975905,
                "law": 0.27906976744186046
            },
            "cat_acc": {
                "STEM": 0.2664015904572565,
                "humanities": 0.2862911795961743,
                "social sciences": 0.30256743581410467,
                "other (business, health, misc.)": 0.3041332510795805
            }
        },
        "toxigen_data": {
            "trans": 0.76,
            "asian": 0.998,
            "black": 0.998,
            "mexican": 0.586,
            "jewish": 0.878,
            "native_american": 0.88,
            "physical_disability": 0.83,
            "lgbtq": 0.656,
            "middle_east": 0.786,
            "women": 0.624,
            "chinese": 0.666,
            "latino": 0.594,
            "mental_disability": 0.936,
            "muslim": 0.916,
            "overall": 0.7934285714285715
        },
        "tydiqa_no_context_data": {
            "indonesian": {
                "exact_match": 9.0,
                "f1": 20.47548295335021
            },
            "finnish": {
                "exact_match": 12.0,
                "f1": 20.71751675954278
            },
            "swahili": {
                "exact_match": 5.0,
                "f1": 7.4171684250631635
            },
            "english": {
                "exact_match": 25.0,
                "f1": 38.645205993866284
            },
            "russian": {
                "exact_match": 2.0,
                "f1": 7.903174603174604
            },
            "bengali": {
                "exact_match": 1.0,
                "f1": 2.733333333333333
            },
            "arabic": {
                "exact_match": 1.0,
                "f1": 3.3673725088121373
            },
            "korean": {
                "exact_match": 9.0,
                "f1": 12.22051282051282
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.28571428571428575
            },
            "average": {
                "f1": 12.640609075929959,
                "exact_match": 7.111111111111111
            }
        },
        "tydiqa_goldp_data": {
            "indonesian": {
                "exact_match": 45.0,
                "f1": 64.63010935324321
            },
            "arabic": {
                "exact_match": 24.0,
                "f1": 44.920803024543076
            },
            "finnish": {
                "exact_match": 44.0,
                "f1": 54.43217354981656
            },
            "bengali": {
                "exact_match": 20.0,
                "f1": 32.299783549783555
            },
            "korean": {
                "exact_match": 45.0,
                "f1": 54.69062414647573
            },
            "english": {
                "exact_match": 47.0,
                "f1": 62.8633506844033
            },
            "russian": {
                "exact_match": 32.0,
                "f1": 49.94164455939265
            },
            "swahili": {
                "exact_match": 26.0,
                "f1": 40.07069232378426
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 1.2857142857142858
            },
            "average": {
                "f1": 45.01498838635073,
                "exact_match": 31.444444444444443
            }
        }
    },
    "lima_filtered": {
        "bbh_direct_data": {
            "word_sorting": 0.2,
            "disambiguation_qa": 0.4,
            "movie_recommendation": 0.325,
            "formal_fallacies": 0.525,
            "causal_judgement": 0.45,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.175,
            "sports_understanding": 0.525,
            "hyperbaton": 0.425,
            "date_understanding": 0.3,
            "web_of_lies": 0.475,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.375,
            "snarks": 0.475,
            "geometric_shapes": 0.3,
            "navigate": 0.525,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.25,
            "salient_translation_error_detection": 0.275,
            "tracking_shuffled_objects_seven_objects": 0.125,
            "tracking_shuffled_objects_five_objects": 0.1,
            "temporal_sequences": 0.075,
            "tracking_shuffled_objects_three_objects": 0.35,
            "logical_deduction_seven_objects": 0.1,
            "object_counting": 0.45,
            "reasoning_about_colored_objects": 0.25,
            "average_exact_match": 0.3259259259259259
        },
        "bbh_cot_data": {
            "word_sorting": 0.075,
            "disambiguation_qa": 0.35,
            "movie_recommendation": 0.7,
            "formal_fallacies": 0.475,
            "causal_judgement": 0.475,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.025,
            "sports_understanding": 0.85,
            "hyperbaton": 0.625,
            "date_understanding": 0.65,
            "web_of_lies": 0.625,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.7,
            "snarks": 0.45,
            "geometric_shapes": 0.275,
            "navigate": 0.575,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.375,
            "salient_translation_error_detection": 0.175,
            "tracking_shuffled_objects_seven_objects": 0.1,
            "tracking_shuffled_objects_five_objects": 0.175,
            "temporal_sequences": 0.125,
            "tracking_shuffled_objects_three_objects": 0.4,
            "logical_deduction_seven_objects": 0.175,
            "object_counting": 0.575,
            "reasoning_about_colored_objects": 0.525,
            "average_exact_match": 0.400925925925926
        },
        "gsm_direct_data": {
            "exact_match": 0.085
        },
        "gsm_cot_data": {
            "exact_match": 0.155
        },
        "mmlu_0_shot_data": {
            "average_acc": 0.32602193419740777,
            "subcat_acc": {
                "math": 0.2640977443609023,
                "health": 0.3378048780487805,
                "physics": 0.2734375,
                "business": 0.3890160183066362,
                "biology": 0.3303964757709251,
                "chemistry": 0.21122112211221122,
                "computer science": 0.33495145631067963,
                "economics": 0.28975741239892183,
                "engineering": 0.31724137931034485,
                "philosophy": 0.308648111332008,
                "other": 0.3605150214592275,
                "history": 0.3634408602150538,
                "geography": 0.3484848484848485,
                "politics": 0.3888888888888889,
                "psychology": 0.36646499567847884,
                "culture": 0.4006024096385542,
                "law": 0.2994895065229722
            },
            "cat_acc": {
                "STEM": 0.2829688535453943,
                "humanities": 0.31604675876726884,
                "social sciences": 0.35521611959701005,
                "other (business, health, misc.)": 0.3528685996298581
            }
        },
        "mmlu_5_shot_data": {
            "average_acc": 0.3074348383421165,
            "subcat_acc": {
                "math": 0.2462406015037594,
                "health": 0.3103658536585366,
                "physics": 0.2625,
                "business": 0.37528604118993136,
                "biology": 0.31718061674008813,
                "chemistry": 0.25742574257425743,
                "computer science": 0.3470873786407767,
                "economics": 0.27088948787061995,
                "engineering": 0.32413793103448274,
                "philosophy": 0.2952286282306163,
                "other": 0.336480686695279,
                "history": 0.3150537634408602,
                "geography": 0.2878787878787879,
                "politics": 0.3533950617283951,
                "psychology": 0.331028522039758,
                "culture": 0.3885542168674699,
                "law": 0.2972206466250709
            },
            "cat_acc": {
                "STEM": 0.27899271040424123,
                "humanities": 0.29989373007438896,
                "social sciences": 0.3246668833279168,
                "other (business, health, misc.)": 0.3285009253547193
            }
        },
        "toxigen_data": {
            "trans": 0.76,
            "asian": 0.998,
            "black": 0.998,
            "mexican": 0.586,
            "jewish": 0.878,
            "native_american": 0.88,
            "physical_disability": 0.83,
            "lgbtq": 0.656,
            "middle_east": 0.786,
            "women": 0.624,
            "chinese": 0.666,
            "latino": 0.594,
            "mental_disability": 0.936,
            "muslim": 0.916,
            "overall": 0.7934285714285715
        },
        "tydiqa_no_context_data": {
            "indonesian": {
                "exact_match": 12.0,
                "f1": 23.891895375830323
            },
            "arabic": {
                "exact_match": 1.0,
                "f1": 6.816030145528246
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.0
            },
            "korean": {
                "exact_match": 16.0,
                "f1": 18.167216117216118
            },
            "swahili": {
                "exact_match": 8.0,
                "f1": 9.662464985994397
            },
            "english": {
                "exact_match": 23.0,
                "f1": 31.405096411436787
            },
            "russian": {
                "exact_match": 6.0,
                "f1": 13.101028138528141
            },
            "bengali": {
                "exact_match": 0.0,
                "f1": 0.28571428571428575
            },
            "finnish": {
                "exact_match": 16.0,
                "f1": 26.262958803004565
            },
            "average": {
                "f1": 14.399156029250321,
                "exact_match": 9.11111111111111
            }
        },
        "tydiqa_goldp_data": {
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.0
            },
            "english": {
                "exact_match": 53.0,
                "f1": 73.30716193820984
            },
            "finnish": {
                "exact_match": 42.0,
                "f1": 52.42794968287388
            },
            "indonesian": {
                "exact_match": 48.0,
                "f1": 62.61116336017043
            },
            "arabic": {
                "exact_match": 28.0,
                "f1": 47.0960469958181
            },
            "bengali": {
                "exact_match": 20.0,
                "f1": 28.38759018759019
            },
            "korean": {
                "exact_match": 50.0,
                "f1": 62.75075132088861
            },
            "swahili": {
                "exact_match": 26.0,
                "f1": 38.28617647058824
            },
            "russian": {
                "exact_match": 27.0,
                "f1": 58.944151913326635
            },
            "average": {
                "f1": 47.09011020771844,
                "exact_match": 32.666666666666664
            }
        }
    },
    "oasst1_filtered": {
        "bbh_direct_data": {
            "word_sorting": 0.2,
            "disambiguation_qa": 0.4,
            "movie_recommendation": 0.325,
            "formal_fallacies": 0.525,
            "causal_judgement": 0.45,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.175,
            "sports_understanding": 0.525,
            "hyperbaton": 0.425,
            "date_understanding": 0.3,
            "web_of_lies": 0.475,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.375,
            "snarks": 0.475,
            "geometric_shapes": 0.3,
            "navigate": 0.525,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.25,
            "salient_translation_error_detection": 0.275,
            "tracking_shuffled_objects_seven_objects": 0.125,
            "tracking_shuffled_objects_five_objects": 0.1,
            "temporal_sequences": 0.075,
            "tracking_shuffled_objects_three_objects": 0.35,
            "logical_deduction_seven_objects": 0.1,
            "object_counting": 0.45,
            "reasoning_about_colored_objects": 0.25,
            "average_exact_match": 0.3259259259259259
        },
        "bbh_cot_data": {
            "word_sorting": 0.075,
            "disambiguation_qa": 0.35,
            "movie_recommendation": 0.7,
            "formal_fallacies": 0.475,
            "causal_judgement": 0.475,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.025,
            "sports_understanding": 0.85,
            "hyperbaton": 0.625,
            "date_understanding": 0.65,
            "web_of_lies": 0.625,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.7,
            "snarks": 0.45,
            "geometric_shapes": 0.275,
            "navigate": 0.575,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.375,
            "salient_translation_error_detection": 0.175,
            "tracking_shuffled_objects_seven_objects": 0.1,
            "tracking_shuffled_objects_five_objects": 0.175,
            "temporal_sequences": 0.125,
            "tracking_shuffled_objects_three_objects": 0.4,
            "logical_deduction_seven_objects": 0.175,
            "object_counting": 0.575,
            "reasoning_about_colored_objects": 0.525,
            "average_exact_match": 0.400925925925926
        },
        "gsm_direct_data": {
            "exact_match": 0.085
        },
        "gsm_cot_data": {
            "exact_match": 0.155
        },
        "mmlu_0_shot_data": {
            "average_acc": 0.3301523999430281,
            "subcat_acc": {
                "math": 0.26127819548872183,
                "health": 0.34207317073170734,
                "physics": 0.2734375,
                "business": 0.3707093821510298,
                "biology": 0.34801762114537443,
                "chemistry": 0.22772277227722773,
                "computer science": 0.3325242718446602,
                "economics": 0.29110512129380056,
                "engineering": 0.31724137931034485,
                "philosophy": 0.3106361829025845,
                "other": 0.37682403433476397,
                "history": 0.36451612903225805,
                "geography": 0.3787878787878788,
                "politics": 0.38425925925925924,
                "psychology": 0.36732929991356955,
                "culture": 0.41566265060240964,
                "law": 0.3085649461145774
            },
            "cat_acc": {
                "STEM": 0.2859509609012591,
                "humanities": 0.32051009564293303,
                "social sciences": 0.3584660383490413,
                "other (business, health, misc.)": 0.3584207279457125
            }
        },
        "mmlu_5_shot_data": {
            "average_acc": 0.32196268337843614,
            "subcat_acc": {
                "math": 0.25093984962406013,
                "health": 0.3298780487804878,
                "physics": 0.259375,
                "business": 0.40045766590389015,
                "biology": 0.32819383259911894,
                "chemistry": 0.25742574257425743,
                "computer science": 0.33495145631067963,
                "economics": 0.29514824797843664,
                "engineering": 0.296551724137931,
                "philosophy": 0.30566600397614313,
                "other": 0.3682403433476395,
                "history": 0.34838709677419355,
                "geography": 0.3686868686868687,
                "politics": 0.39814814814814814,
                "psychology": 0.3388072601555748,
                "culture": 0.4006024096385542,
                "law": 0.2955190017016449
            },
            "cat_acc": {
                "STEM": 0.27866136514247847,
                "humanities": 0.310308182784272,
                "social sciences": 0.3493662658433539,
                "other (business, health, misc.)": 0.3531770512029611
            }
        },
        "toxigen_data": {
            "trans": 0.76,
            "asian": 0.998,
            "black": 0.998,
            "mexican": 0.586,
            "jewish": 0.878,
            "native_american": 0.88,
            "physical_disability": 0.83,
            "lgbtq": 0.656,
            "middle_east": 0.786,
            "women": 0.624,
            "chinese": 0.666,
            "latino": 0.594,
            "mental_disability": 0.936,
            "muslim": 0.916,
            "overall": 0.7934285714285715
        },
        "tydiqa_no_context_data": {
            "arabic": {
                "exact_match": 0.0,
                "f1": 3.967159682873627
            },
            "english": {
                "exact_match": 26.0,
                "f1": 34.33245758073345
            },
            "finnish": {
                "exact_match": 10.0,
                "f1": 16.20198787246053
            },
            "bengali": {
                "exact_match": 1.0,
                "f1": 1.685714285714286
            },
            "korean": {
                "exact_match": 13.0,
                "f1": 15.992553191489362
            },
            "swahili": {
                "exact_match": 4.0,
                "f1": 4.7630081300813005
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 7.285714285714286
            },
            "russian": {
                "exact_match": 4.0,
                "f1": 13.773593073593071
            },
            "indonesian": {
                "exact_match": 11.0,
                "f1": 24.041664860569522
            },
            "average": {
                "f1": 13.560428107025494,
                "exact_match": 7.666666666666667
            }
        },
        "tydiqa_goldp_data": {
            "russian": {
                "exact_match": 29.0,
                "f1": 48.91549681312838
            },
            "korean": {
                "exact_match": 51.0,
                "f1": 62.233852258852245
            },
            "finnish": {
                "exact_match": 46.0,
                "f1": 57.30926889178258
            },
            "arabic": {
                "exact_match": 14.0,
                "f1": 40.93764817783741
            },
            "swahili": {
                "exact_match": 25.0,
                "f1": 41.21497542932467
            },
            "indonesian": {
                "exact_match": 40.0,
                "f1": 61.00287948070615
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.25
            },
            "bengali": {
                "exact_match": 20.0,
                "f1": 31.201948051948047
            },
            "english": {
                "exact_match": 46.0,
                "f1": 64.36466003669224
            },
            "average": {
                "f1": 45.27008101558575,
                "exact_match": 30.11111111111111
            }
        }
    },
    "open_orca_filtered": {
        "bbh_direct_data": {
            "word_sorting": 0.2,
            "disambiguation_qa": 0.4,
            "movie_recommendation": 0.325,
            "formal_fallacies": 0.525,
            "causal_judgement": 0.45,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.175,
            "sports_understanding": 0.525,
            "hyperbaton": 0.425,
            "date_understanding": 0.3,
            "web_of_lies": 0.475,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.375,
            "snarks": 0.475,
            "geometric_shapes": 0.3,
            "navigate": 0.525,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.25,
            "salient_translation_error_detection": 0.275,
            "tracking_shuffled_objects_seven_objects": 0.125,
            "tracking_shuffled_objects_five_objects": 0.1,
            "temporal_sequences": 0.075,
            "tracking_shuffled_objects_three_objects": 0.35,
            "logical_deduction_seven_objects": 0.1,
            "object_counting": 0.45,
            "reasoning_about_colored_objects": 0.25,
            "average_exact_match": 0.3259259259259259
        },
        "bbh_cot_data": {
            "word_sorting": 0.075,
            "disambiguation_qa": 0.35,
            "movie_recommendation": 0.7,
            "formal_fallacies": 0.475,
            "causal_judgement": 0.475,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.025,
            "sports_understanding": 0.85,
            "hyperbaton": 0.625,
            "date_understanding": 0.65,
            "web_of_lies": 0.625,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.7,
            "snarks": 0.45,
            "geometric_shapes": 0.275,
            "navigate": 0.575,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.375,
            "salient_translation_error_detection": 0.175,
            "tracking_shuffled_objects_seven_objects": 0.1,
            "tracking_shuffled_objects_five_objects": 0.175,
            "temporal_sequences": 0.125,
            "tracking_shuffled_objects_three_objects": 0.4,
            "logical_deduction_seven_objects": 0.175,
            "object_counting": 0.575,
            "reasoning_about_colored_objects": 0.525,
            "average_exact_match": 0.400925925925926
        },
        "gsm_direct_data": {
            "exact_match": 0.085
        },
        "gsm_cot_data": {
            "exact_match": 0.155
        },
        "mmlu_0_shot_data": {
            "average_acc": 0.3570004272895599,
            "subcat_acc": {
                "math": 0.26597744360902253,
                "health": 0.3591463414634146,
                "physics": 0.2703125,
                "business": 0.4302059496567506,
                "biology": 0.3832599118942731,
                "chemistry": 0.23432343234323433,
                "computer science": 0.3907766990291262,
                "economics": 0.29245283018867924,
                "engineering": 0.35172413793103446,
                "philosophy": 0.33101391650099404,
                "other": 0.41030042918454934,
                "history": 0.4311827956989247,
                "geography": 0.4090909090909091,
                "politics": 0.4444444444444444,
                "psychology": 0.4062229904926534,
                "culture": 0.4608433734939759,
                "law": 0.3227453204764606
            },
            "cat_acc": {
                "STEM": 0.30251822398939693,
                "humanities": 0.3477151965993624,
                "social sciences": 0.392915177120572,
                "other (business, health, misc.)": 0.38710672424429365
            }
        },
        "mmlu_5_shot_data": {
            "average_acc": 0.353938185443669,
            "subcat_acc": {
                "math": 0.2706766917293233,
                "health": 0.3603658536585366,
                "physics": 0.303125,
                "business": 0.4233409610983982,
                "biology": 0.35462555066079293,
                "chemistry": 0.23102310231023102,
                "computer science": 0.3713592233009709,
                "economics": 0.2816711590296496,
                "engineering": 0.3724137931034483,
                "philosophy": 0.32554671968190857,
                "other": 0.4094420600858369,
                "history": 0.42473118279569894,
                "geography": 0.41919191919191917,
                "politics": 0.45987654320987653,
                "psychology": 0.4079515989628349,
                "culture": 0.42168674698795183,
                "law": 0.3091321610890527
            },
            "cat_acc": {
                "STEM": 0.30483764082173626,
                "humanities": 0.3390010626992561,
                "social sciences": 0.39064023399415015,
                "other (business, health, misc.)": 0.3864898210980876
            }
        },
        "toxigen_data": {
            "trans": 0.76,
            "asian": 0.998,
            "black": 0.998,
            "mexican": 0.586,
            "jewish": 0.878,
            "native_american": 0.88,
            "physical_disability": 0.83,
            "lgbtq": 0.656,
            "middle_east": 0.786,
            "women": 0.624,
            "chinese": 0.666,
            "latino": 0.594,
            "mental_disability": 0.936,
            "muslim": 0.916,
            "overall": 0.7934285714285715
        },
        "tydiqa_no_context_data": {
            "indonesian": {
                "exact_match": 10.0,
                "f1": 22.437632021025003
            },
            "arabic": {
                "exact_match": 0.0,
                "f1": 6.508542678215467
            },
            "korean": {
                "exact_match": 10.0,
                "f1": 16.870289855072464
            },
            "english": {
                "exact_match": 28.0,
                "f1": 39.760913467120346
            },
            "russian": {
                "exact_match": 3.0,
                "f1": 7.795238095238096
            },
            "swahili": {
                "exact_match": 7.0,
                "f1": 8.039860566176355
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.28571428571428575
            },
            "finnish": {
                "exact_match": 15.0,
                "f1": 21.337033069058233
            },
            "bengali": {
                "exact_match": 1.0,
                "f1": 1.685714285714286
            },
            "average": {
                "f1": 13.85788203592606,
                "exact_match": 8.222222222222221
            }
        },
        "tydiqa_goldp_data": {
            "indonesian": {
                "exact_match": 39.0,
                "f1": 62.90214188257563
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 2.1857142857142855
            },
            "arabic": {
                "exact_match": 23.0,
                "f1": 45.48565076719468
            },
            "swahili": {
                "exact_match": 1.0,
                "f1": 14.404203191216586
            },
            "russian": {
                "exact_match": 32.0,
                "f1": 54.22084680025855
            },
            "finnish": {
                "exact_match": 36.0,
                "f1": 52.85119644944034
            },
            "korean": {
                "exact_match": 49.0,
                "f1": 61.90630687644418
            },
            "bengali": {
                "exact_match": 16.0,
                "f1": 28.346031746031755
            },
            "english": {
                "exact_match": 59.0,
                "f1": 72.83206976449596
            },
            "average": {
                "f1": 43.903795751485774,
                "exact_match": 28.333333333333332
            }
        }
    },
    "science_filtered": {
        "bbh_direct_data": {
            "word_sorting": 0.2,
            "disambiguation_qa": 0.4,
            "movie_recommendation": 0.325,
            "formal_fallacies": 0.525,
            "causal_judgement": 0.45,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.175,
            "sports_understanding": 0.525,
            "hyperbaton": 0.425,
            "date_understanding": 0.3,
            "web_of_lies": 0.475,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.375,
            "snarks": 0.475,
            "geometric_shapes": 0.3,
            "navigate": 0.525,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.25,
            "salient_translation_error_detection": 0.275,
            "tracking_shuffled_objects_seven_objects": 0.125,
            "tracking_shuffled_objects_five_objects": 0.1,
            "temporal_sequences": 0.075,
            "tracking_shuffled_objects_three_objects": 0.35,
            "logical_deduction_seven_objects": 0.1,
            "object_counting": 0.45,
            "reasoning_about_colored_objects": 0.25,
            "average_exact_match": 0.3259259259259259
        },
        "bbh_cot_data": {
            "word_sorting": 0.075,
            "disambiguation_qa": 0.35,
            "movie_recommendation": 0.7,
            "formal_fallacies": 0.475,
            "causal_judgement": 0.475,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.025,
            "sports_understanding": 0.85,
            "hyperbaton": 0.625,
            "date_understanding": 0.65,
            "web_of_lies": 0.625,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.7,
            "snarks": 0.45,
            "geometric_shapes": 0.275,
            "navigate": 0.575,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.375,
            "salient_translation_error_detection": 0.175,
            "tracking_shuffled_objects_seven_objects": 0.1,
            "tracking_shuffled_objects_five_objects": 0.175,
            "temporal_sequences": 0.125,
            "tracking_shuffled_objects_three_objects": 0.4,
            "logical_deduction_seven_objects": 0.175,
            "object_counting": 0.575,
            "reasoning_about_colored_objects": 0.525,
            "average_exact_match": 0.400925925925926
        },
        "gsm_direct_data": {
            "exact_match": 0.085
        },
        "gsm_cot_data": {
            "exact_match": 0.155
        },
        "mmlu_0_shot_data": {
            "average_acc": 0.3148411907135736,
            "subcat_acc": {
                "math": 0.2565789473684211,
                "health": 0.3207317073170732,
                "physics": 0.28125,
                "business": 0.38443935926773454,
                "biology": 0.33480176211453744,
                "chemistry": 0.24092409240924093,
                "computer science": 0.32524271844660196,
                "economics": 0.2816711590296496,
                "engineering": 0.296551724137931,
                "philosophy": 0.28578528827037775,
                "other": 0.35536480686695276,
                "history": 0.34301075268817205,
                "geography": 0.3181818181818182,
                "politics": 0.37808641975308643,
                "psychology": 0.3526361279170268,
                "culture": 0.3614457831325301,
                "law": 0.2943845717526943
            },
            "cat_acc": {
                "STEM": 0.28330019880715707,
                "humanities": 0.3003188097768332,
                "social sciences": 0.3396165095872603,
                "other (business, health, misc.)": 0.3417643429981493
            }
        },
        "mmlu_5_shot_data": {
            "average_acc": 0.3356359492949722,
            "subcat_acc": {
                "math": 0.2575187969924812,
                "health": 0.3475609756097561,
                "physics": 0.284375,
                "business": 0.41418764302059496,
                "biology": 0.3568281938325991,
                "chemistry": 0.25412541254125415,
                "computer science": 0.3422330097087379,
                "economics": 0.29110512129380056,
                "engineering": 0.3310344827586207,
                "philosophy": 0.3225646123260437,
                "other": 0.3785407725321888,
                "history": 0.36774193548387096,
                "geography": 0.3888888888888889,
                "politics": 0.4027777777777778,
                "psychology": 0.3690579083837511,
                "culture": 0.39759036144578314,
                "law": 0.3023255813953488
            },
            "cat_acc": {
                "STEM": 0.292909211398277,
                "humanities": 0.3239107332624867,
                "social sciences": 0.36171595710107246,
                "other (business, health, misc.)": 0.3676742751388032
            }
        },
        "toxigen_data": {
            "trans": 0.76,
            "asian": 0.998,
            "black": 0.998,
            "mexican": 0.586,
            "jewish": 0.878,
            "native_american": 0.88,
            "physical_disability": 0.83,
            "lgbtq": 0.656,
            "middle_east": 0.786,
            "women": 0.624,
            "chinese": 0.666,
            "latino": 0.594,
            "mental_disability": 0.936,
            "muslim": 0.916,
            "overall": 0.7934285714285715
        },
        "tydiqa_no_context_data": {
            "arabic": {
                "exact_match": 1.0,
                "f1": 4.181648682004045
            },
            "bengali": {
                "exact_match": 1.0,
                "f1": 1.5357142857142858
            },
            "swahili": {
                "exact_match": 12.0,
                "f1": 12.678894205209994
            },
            "english": {
                "exact_match": 24.0,
                "f1": 40.98651776794633
            },
            "russian": {
                "exact_match": 3.0,
                "f1": 9.578724053724054
            },
            "finnish": {
                "exact_match": 14.0,
                "f1": 20.30324430045198
            },
            "korean": {
                "exact_match": 14.0,
                "f1": 16.95695652173913
            },
            "indonesian": {
                "exact_match": 8.0,
                "f1": 19.661120477990064
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.0
            },
            "average": {
                "f1": 13.98698003275332,
                "exact_match": 8.555555555555555
            }
        },
        "tydiqa_goldp_data": {
            "finnish": {
                "exact_match": 47.0,
                "f1": 61.617301168512896
            },
            "swahili": {
                "exact_match": 36.0,
                "f1": 44.890346495609634
            },
            "indonesian": {
                "exact_match": 40.0,
                "f1": 63.86246233071776
            },
            "english": {
                "exact_match": 45.0,
                "f1": 65.49854930644403
            },
            "bengali": {
                "exact_match": 22.0,
                "f1": 30.547186147186142
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.33333333333333326
            },
            "arabic": {
                "exact_match": 27.0,
                "f1": 49.05012562541067
            },
            "russian": {
                "exact_match": 28.0,
                "f1": 41.458646616541344
            },
            "korean": {
                "exact_match": 54.0,
                "f1": 65.12587782587782
            },
            "average": {
                "f1": 46.93153653884818,
                "exact_match": 33.22222222222222
            }
        }
    },
    "sharegpt_filtered": {
        "bbh_direct_data": {
            "word_sorting": 0.2,
            "disambiguation_qa": 0.4,
            "movie_recommendation": 0.325,
            "formal_fallacies": 0.525,
            "causal_judgement": 0.45,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.175,
            "sports_understanding": 0.525,
            "hyperbaton": 0.425,
            "date_understanding": 0.3,
            "web_of_lies": 0.475,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.375,
            "snarks": 0.475,
            "geometric_shapes": 0.3,
            "navigate": 0.525,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.25,
            "salient_translation_error_detection": 0.275,
            "tracking_shuffled_objects_seven_objects": 0.125,
            "tracking_shuffled_objects_five_objects": 0.1,
            "temporal_sequences": 0.075,
            "tracking_shuffled_objects_three_objects": 0.35,
            "logical_deduction_seven_objects": 0.1,
            "object_counting": 0.45,
            "reasoning_about_colored_objects": 0.25,
            "average_exact_match": 0.3259259259259259
        },
        "bbh_cot_data": {
            "word_sorting": 0.075,
            "disambiguation_qa": 0.35,
            "movie_recommendation": 0.7,
            "formal_fallacies": 0.475,
            "causal_judgement": 0.475,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.025,
            "sports_understanding": 0.85,
            "hyperbaton": 0.625,
            "date_understanding": 0.65,
            "web_of_lies": 0.625,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.7,
            "snarks": 0.45,
            "geometric_shapes": 0.275,
            "navigate": 0.575,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.375,
            "salient_translation_error_detection": 0.175,
            "tracking_shuffled_objects_seven_objects": 0.1,
            "tracking_shuffled_objects_five_objects": 0.175,
            "temporal_sequences": 0.125,
            "tracking_shuffled_objects_three_objects": 0.4,
            "logical_deduction_seven_objects": 0.175,
            "object_counting": 0.575,
            "reasoning_about_colored_objects": 0.525,
            "average_exact_match": 0.400925925925926
        },
        "gsm_direct_data": {
            "exact_match": 0.085
        },
        "gsm_cot_data": {
            "exact_match": 0.155
        },
        "mmlu_0_shot_data": {
            "average_acc": 0.3562882780230736,
            "subcat_acc": {
                "math": 0.2575187969924812,
                "health": 0.37682926829268293,
                "physics": 0.290625,
                "business": 0.43935926773455375,
                "biology": 0.4118942731277533,
                "chemistry": 0.2508250825082508,
                "computer science": 0.3567961165048544,
                "economics": 0.29245283018867924,
                "engineering": 0.3310344827586207,
                "philosophy": 0.3349900596421471,
                "other": 0.4034334763948498,
                "history": 0.4064516129032258,
                "geography": 0.4090909090909091,
                "politics": 0.42746913580246915,
                "psychology": 0.41227312013828865,
                "culture": 0.4307228915662651,
                "law": 0.316505955757232
            },
            "cat_acc": {
                "STEM": 0.30417495029821073,
                "humanities": 0.3421891604675877,
                "social sciences": 0.3883652908677283,
                "other (business, health, misc.)": 0.39481801357186924
            }
        },
        "mmlu_5_shot_data": {
            "average_acc": 0.35635949294972225,
            "subcat_acc": {
                "math": 0.2575187969924812,
                "health": 0.36341463414634145,
                "physics": 0.2875,
                "business": 0.4576659038901602,
                "biology": 0.44493392070484583,
                "chemistry": 0.3201320132013201,
                "computer science": 0.32524271844660196,
                "economics": 0.3018867924528302,
                "engineering": 0.27586206896551724,
                "philosophy": 0.3379721669980119,
                "other": 0.48497854077253216,
                "history": 0.3956989247311828,
                "geography": 0.398989898989899,
                "politics": 0.43209876543209874,
                "psychology": 0.3915298184961106,
                "culture": 0.42771084337349397,
                "law": 0.2756664775950085
            },
            "cat_acc": {
                "STEM": 0.30848243870112657,
                "humanities": 0.32603613177470775,
                "social sciences": 0.3828404289892753,
                "other (business, health, misc.)": 0.41980259099321404
            }
        },
        "toxigen_data": {
            "trans": 0.76,
            "asian": 0.998,
            "black": 0.998,
            "mexican": 0.586,
            "jewish": 0.878,
            "native_american": 0.88,
            "physical_disability": 0.83,
            "lgbtq": 0.656,
            "middle_east": 0.786,
            "women": 0.624,
            "chinese": 0.666,
            "latino": 0.594,
            "mental_disability": 0.936,
            "muslim": 0.916,
            "overall": 0.7934285714285715
        },
        "tydiqa_no_context_data": {
            "arabic": {
                "exact_match": 0.0,
                "f1": 3.1213355084832157
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.5714285714285715
            },
            "finnish": {
                "exact_match": 9.0,
                "f1": 17.004058820360353
            },
            "english": {
                "exact_match": 26.0,
                "f1": 38.60253800476837
            },
            "bengali": {
                "exact_match": 1.0,
                "f1": 1.9079365079365078
            },
            "russian": {
                "exact_match": 2.0,
                "f1": 9.144934640522877
            },
            "indonesian": {
                "exact_match": 7.0,
                "f1": 18.052573821089467
            },
            "korean": {
                "exact_match": 15.0,
                "f1": 19.690196078431374
            },
            "swahili": {
                "exact_match": 4.0,
                "f1": 5.0
            },
            "average": {
                "f1": 12.566111328113415,
                "exact_match": 7.111111111111111
            }
        },
        "tydiqa_goldp_data": {
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.0
            },
            "english": {
                "exact_match": 47.0,
                "f1": 66.89000027880765
            },
            "arabic": {
                "exact_match": 27.0,
                "f1": 47.30934301689032
            },
            "bengali": {
                "exact_match": 17.0,
                "f1": 27.890476190476196
            },
            "korean": {
                "exact_match": 7.0,
                "f1": 18.34792768253867
            },
            "swahili": {
                "exact_match": 38.0,
                "f1": 48.72750669777893
            },
            "russian": {
                "exact_match": 34.0,
                "f1": 52.96634275974303
            },
            "indonesian": {
                "exact_match": 44.0,
                "f1": 57.91779900398263
            },
            "finnish": {
                "exact_match": 43.0,
                "f1": 57.3016969164522
            },
            "average": {
                "f1": 41.92789917185218,
                "exact_match": 28.555555555555557
            }
        }
    },
    "wizardlm_filtered": {
        "bbh_direct_data": {
            "word_sorting": 0.2,
            "disambiguation_qa": 0.4,
            "movie_recommendation": 0.325,
            "formal_fallacies": 0.525,
            "causal_judgement": 0.45,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.175,
            "sports_understanding": 0.525,
            "hyperbaton": 0.425,
            "date_understanding": 0.3,
            "web_of_lies": 0.475,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.375,
            "snarks": 0.475,
            "geometric_shapes": 0.3,
            "navigate": 0.525,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.25,
            "salient_translation_error_detection": 0.275,
            "tracking_shuffled_objects_seven_objects": 0.125,
            "tracking_shuffled_objects_five_objects": 0.1,
            "temporal_sequences": 0.075,
            "tracking_shuffled_objects_three_objects": 0.35,
            "logical_deduction_seven_objects": 0.1,
            "object_counting": 0.45,
            "reasoning_about_colored_objects": 0.25,
            "average_exact_match": 0.3259259259259259
        },
        "bbh_cot_data": {
            "word_sorting": 0.075,
            "disambiguation_qa": 0.35,
            "movie_recommendation": 0.7,
            "formal_fallacies": 0.475,
            "causal_judgement": 0.475,
            "multistep_arithmetic_two": 0.0,
            "dyck_languages": 0.025,
            "sports_understanding": 0.85,
            "hyperbaton": 0.625,
            "date_understanding": 0.65,
            "web_of_lies": 0.625,
            "boolean_expressions": 0.675,
            "logical_deduction_three_objects": 0.7,
            "snarks": 0.45,
            "geometric_shapes": 0.275,
            "navigate": 0.575,
            "penguins_in_a_table": 0.375,
            "ruin_names": 0.3,
            "logical_deduction_five_objects": 0.375,
            "salient_translation_error_detection": 0.175,
            "tracking_shuffled_objects_seven_objects": 0.1,
            "tracking_shuffled_objects_five_objects": 0.175,
            "temporal_sequences": 0.125,
            "tracking_shuffled_objects_three_objects": 0.4,
            "logical_deduction_seven_objects": 0.175,
            "object_counting": 0.575,
            "reasoning_about_colored_objects": 0.525,
            "average_exact_match": 0.400925925925926
        },
        "gsm_direct_data": {
            "exact_match": 0.085
        },
        "gsm_cot_data": {
            "exact_match": 0.155
        },
        "mmlu_0_shot_data": {
            "average_acc": 0.3399088448938898,
            "subcat_acc": {
                "math": 0.25093984962406013,
                "health": 0.3463414634146341,
                "physics": 0.2734375,
                "business": 0.39816933638443935,
                "biology": 0.34801762114537443,
                "chemistry": 0.20462046204620463,
                "computer science": 0.3567961165048544,
                "economics": 0.29110512129380056,
                "engineering": 0.3103448275862069,
                "philosophy": 0.3290258449304175,
                "other": 0.3811158798283262,
                "history": 0.4075268817204301,
                "geography": 0.3888888888888889,
                "politics": 0.39197530864197533,
                "psychology": 0.38461538461538464,
                "culture": 0.43373493975903615,
                "law": 0.3153715258082813
            },
            "cat_acc": {
                "STEM": 0.2829688535453943,
                "humanities": 0.3394261424017003,
                "social sciences": 0.36919077023074426,
                "other (business, health, misc.)": 0.3658235657001851
            }
        },
        "mmlu_5_shot_data": {
            "average_acc": 0.33428286568864835,
            "subcat_acc": {
                "math": 0.26127819548872183,
                "health": 0.35548780487804876,
                "physics": 0.25625,
                "business": 0.391304347826087,
                "biology": 0.33480176211453744,
                "chemistry": 0.2607260726072607,
                "computer science": 0.3567961165048544,
                "economics": 0.29784366576819404,
                "engineering": 0.3448275862068966,
                "philosophy": 0.3205765407554672,
                "other": 0.37510729613733906,
                "history": 0.3774193548387097,
                "geography": 0.398989898989899,
                "politics": 0.39814814814814814,
                "psychology": 0.3612791702679343,
                "culture": 0.42168674698795183,
                "law": 0.2955190017016449
            },
            "cat_acc": {
                "STEM": 0.2882703777335984,
                "humanities": 0.322422954303932,
                "social sciences": 0.36269093272668185,
                "other (business, health, misc.)": 0.3673658235657002
            }
        },
        "toxigen_data": {
            "trans": 0.76,
            "asian": 0.998,
            "black": 0.998,
            "mexican": 0.586,
            "jewish": 0.878,
            "native_american": 0.88,
            "physical_disability": 0.83,
            "lgbtq": 0.656,
            "middle_east": 0.786,
            "women": 0.624,
            "chinese": 0.666,
            "latino": 0.594,
            "mental_disability": 0.936,
            "muslim": 0.916,
            "overall": 0.7934285714285715
        },
        "tydiqa_no_context_data": {
            "arabic": {
                "exact_match": 1.0,
                "f1": 4.132565657086941
            },
            "english": {
                "exact_match": 25.0,
                "f1": 35.26817042606516
            },
            "finnish": {
                "exact_match": 10.0,
                "f1": 15.417305654797008
            },
            "russian": {
                "exact_match": 8.0,
                "f1": 14.51690421071226
            },
            "indonesian": {
                "exact_match": 15.0,
                "f1": 25.06542829775588
            },
            "bengali": {
                "exact_match": 1.0,
                "f1": 2.2412698412698413
            },
            "korean": {
                "exact_match": 18.0,
                "f1": 23.06363636363636
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.0
            },
            "swahili": {
                "exact_match": 5.0,
                "f1": 7.495115995115996
            },
            "average": {
                "f1": 14.133377382937717,
                "exact_match": 9.222222222222221
            }
        },
        "tydiqa_goldp_data": {
            "arabic": {
                "exact_match": 24.0,
                "f1": 41.31084237459791
            },
            "swahili": {
                "exact_match": 26.0,
                "f1": 42.63793668319984
            },
            "korean": {
                "exact_match": 47.0,
                "f1": 57.24169164169163
            },
            "russian": {
                "exact_match": 26.0,
                "f1": 49.368254998415544
            },
            "telugu": {
                "exact_match": 0.0,
                "f1": 0.6666666666666665
            },
            "bengali": {
                "exact_match": 21.0,
                "f1": 30.389754689754685
            },
            "english": {
                "exact_match": 49.0,
                "f1": 67.04415522285126
            },
            "finnish": {
                "exact_match": 29.0,
                "f1": 39.95248488479514
            },
            "indonesian": {
                "exact_match": 35.0,
                "f1": 59.184379493605576
            },
            "average": {
                "f1": 43.088462961730926,
                "exact_match": 28.555555555555557
            }
        }
    }
}