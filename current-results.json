{'llama-2-7b': {'bbh_direct_data': {'word_sorting': 0.225, 'disambiguation_qa': 0.4, 'movie_recommendation': 0.35, 'formal_fallacies': 0.525, 'causal_judgement': 0.45, 'multistep_arithmetic_two': 0.025, 'dyck_languages': 0.275, 'sports_understanding': 0.7, 'hyperbaton': 0.425, 'date_understanding': 0.325, 'web_of_lies': 0.475, 'boolean_expressions': 0.6, 'logical_deduction_three_objects': 0.45, 'snarks': 0.475, 'geometric_shapes': 0.1, 'navigate': 0.475, 'penguins_in_a_table': 0.275, 'ruin_names': 0.25, 'logical_deduction_five_objects': 0.175, 'salient_translation_error_detection': 0.275, 'tracking_shuffled_objects_seven_objects': 0.05, 'tracking_shuffled_objects_five_objects': 0.1, 'temporal_sequences': 0.125, 'tracking_shuffled_objects_three_objects': 0.35, 'logical_deduction_seven_objects': 0.1, 'object_counting': 0.35, 'reasoning_about_colored_objects': 0.275, 'average_exact_match': 0.3185185185185184}, 'bbh_cot_data': {'word_sorting': 0.05, 'disambiguation_qa': 0.325, 'movie_recommendation': 0.725, 'formal_fallacies': 0.475, 'causal_judgement': 0.525, 'multistep_arithmetic_two': 0.0, 'dyck_languages': 0.1, 'sports_understanding': 0.875, 'hyperbaton': 0.55, 'date_understanding': 0.65, 'web_of_lies': 0.55, 'boolean_expressions': 0.7, 'logical_deduction_three_objects': 0.675, 'snarks': 0.475, 'geometric_shapes': 0.325, 'navigate': 0.45, 'penguins_in_a_table': 0.35, 'ruin_names': 0.325, 'logical_deduction_five_objects': 0.325, 'salient_translation_error_detection': 0.15, 'tracking_shuffled_objects_seven_objects': 0.075, 'tracking_shuffled_objects_five_objects': 0.1, 'temporal_sequences': 0.175, 'tracking_shuffled_objects_three_objects': 0.35, 'logical_deduction_seven_objects': 0.225, 'object_counting': 0.55, 'reasoning_about_colored_objects': 0.525, 'average_exact_match': 0.3925925925925926}, 'gsm_direct_data': {'exact_match': 0.08}, 'gsm_cot_data': {'exact_match': 0.12}, 'mmlu_0_shot_data': {'average_acc': 0.41589517162797324, 'subcat_acc': {'math': 0.2640977443609023, 'health': 0.42317073170731706, 'physics': 0.3328125, 'business': 0.5423340961098398, 'biology': 0.45594713656387664, 'chemistry': 0.31683168316831684, 'computer science': 0.41504854368932037, 'economics': 0.37870619946091644, 'engineering': 0.4, 'philosophy': 0.3523856858846918, 'other': 0.48068669527896996, 'history': 0.5408602150537635, 'geography': 0.4494949494949495, 'politics': 0.5138888888888888, 'psychology': 0.49265341400172863, 'culture': 0.6024096385542169, 'law': 0.3618831537152581}, 'cat_acc': {'STEM': 0.3399602385685885, 'humanities': 0.3931987247608927, 'social sciences': 0.47871303217419564, 'other (business, health, misc.)': 0.459901295496607}}, 'mmlu_5_shot_data': {'average_acc': 0.45919384703033755, 'subcat_acc': {'math': 0.2857142857142857, 'health': 0.48841463414634145, 'physics': 0.3609375, 'business': 0.6155606407322655, 'biology': 0.4955947136563877, 'chemistry': 0.36303630363036304, 'computer science': 0.42961165048543687, 'economics': 0.42587601078167114, 'engineering': 0.4827586206896552, 'philosophy': 0.40457256461232605, 'other': 0.542489270386266, 'history': 0.5688172043010753, 'geography': 0.4898989898989899, 'politics': 0.5740740740740741, 'psychology': 0.5280898876404494, 'culture': 0.5963855421686747, 'law': 0.39251276233692567}, 'cat_acc': {'STEM': 0.37011265738899934, 'humanities': 0.43251859723698194, 'social sciences': 0.5180370490737731, 'other (business, health, misc.)': 0.5249845774213449}}, 'toxigen_data': {'trans': 0.852, 'asian': 0.51, 'black': 0.968, 'mexican': 0.922, 'jewish': 0.284, 'native_american': 0.944, 'physical_disability': 0.904, 'lgbtq': 0.786, 'middle_east': 0.938, 'women': 0.686, 'chinese': 0.868, 'latino': 0.326, 'mental_disability': 0.954, 'muslim': 0.886, 'overall': 0.7734285714285715}, 'tydiqa_no_context_data': {'finnish': {'exact_match': 15.0, 'f1': 23.79913610047754}, 'arabic': {'exact_match': 0.0, 'f1': 4.691079855741991}, 'korean': {'exact_match': 15.0, 'f1': 20.503623188405797}, 'swahili': {'exact_match': 9.0, 'f1': 11.014163614163614}, 'bengali': {'exact_match': 0.0, 'f1': 0.28571428571428575}, 'telugu': {'exact_match': 0.0, 'f1': 0.33333333333333326}, 'english': {'exact_match': 19.0, 'f1': 36.625079911987434}, 'indonesian': {'exact_match': 9.0, 'f1': 19.331547755355803}, 'russian': {'exact_match': 7.0, 'f1': 16.781768231768233}, 'average': {'f1': 14.818382919660893, 'exact_match': 8.222222222222221}}, 'tydiqa_goldp_data': {'korean': {'exact_match': 64.0, 'f1': 70.30940170940171}, 'swahili': {'exact_match': 41.0, 'f1': 50.46222699606156}, 'indonesian': {'exact_match': 42.0, 'f1': 58.27846172985203}, 'telugu': {'exact_match': 0.0, 'f1': 0.6}, 'finnish': {'exact_match': 45.0, 'f1': 52.9303105492896}, 'english': {'exact_match': 46.0, 'f1': 63.67836231565045}, 'russian': {'exact_match': 38.0, 'f1': 54.12774337805296}, 'arabic': {'exact_match': 34.0, 'f1': 56.80496991507357}, 'bengali': {'exact_match': 20.0, 'f1': 27.540476190476188}, 'average': {'f1': 48.30355030931756, 'exact_match': 36.666666666666664}}}}