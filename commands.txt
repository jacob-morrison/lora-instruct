accelerate launch --mixed_precision bf16 --num_machines 1 --num_processes 4 \
    open_instruct/finetune_trainer.py --model_name_or_path /net/nfs.cirrascale/allennlp/yizhongw/hf_llama2_models/7B \
    --use_flash_attn --use_lora --lora_rank 256 --lora_alpha 256 --lora_dropout 0.05 \
    --tokenizer_name /net/nfs.cirrascale/allennlp/yizhongw/hf_llama2_models/7B --use_slow_tokenizer \
    --train_file /net/nfs.cirrascale/allennlp/hamishi/open-instruct/tulu_data/tulu_v1_mix.jsonl \
    --max_seq_length 2048 --preprocessing_num_workers 16 --checkpointing_steps epoch \
    --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --learning_rate 1e-4 \
    --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0. --num_train_epochs 3 \
    --output_dir /output/lora_models --with_tracking --report_to tensorboard --logging_steps 1 \
    && python3 open_instruct/merge_lora.py \
    --base_model_name_or_path /net/nfs.cirrascale/allennlp/yizhongw/hf_llama2_models/7B \
    --lora_model_name_or_path /output/lora_models --output_dir /output/lora_models_merged \
    && python3 -m eval.mmlu.run_eval --ntrain 5 \
    --data_dir /net/nfs.cirrascale/allennlp/yizhongw/open-instruct/data/eval/mmlu \
    --save_dir /output/mmlu_results --model_name_or_path /output/lora_models_merged \
    --tokenizer_name_or_path /net/nfs.cirrascale/allennlp/yizhongw/hf_llama2_models/7B \
    --eval_batch_size 8 --use_chat_format
